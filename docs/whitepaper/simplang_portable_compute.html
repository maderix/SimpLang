<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SimpLang: What if CUDA Wasn't Locked to NVIDIA?</title>
    <style>
        /* Karpathy-inspired clean, readable style */
        :root {
            --text-color: #333;
            --code-bg: #f6f8fa;
            --border-color: #d1d5da;
            --link-color: #0366d6;
            --accent: #6f42c1;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            max-width: 850px;
            margin: 0 auto;
            padding: 3rem 1.5rem;
            background: white;
        }

        /* Typography */
        h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            font-weight: 700;
            letter-spacing: -0.02em;
        }

        .subtitle {
            font-size: 1.3rem;
            color: #666;
            margin-bottom: 2rem;
            font-weight: 400;
        }

        h2 {
            font-size: 1.8rem;
            margin-top: 3rem;
            margin-bottom: 1rem;
            font-weight: 600;
        }

        h3 {
            font-size: 1.3rem;
            margin-top: 2rem;
            margin-bottom: 0.8rem;
            font-weight: 600;
        }

        p {
            margin-bottom: 1.2rem;
        }

        .highlight-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 2rem 0;
            font-size: 1.1rem;
        }

        .insight-box {
            background: #fff9e6;
            border-left: 4px solid #f9a825;
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
        }

        /* Code blocks */
        pre {
            background: var(--code-bg);
            border: 1px solid var(--border-color);
            border-radius: 6px;
            padding: 1rem;
            overflow-x: auto;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        code {
            font-family: 'SF Mono', Consolas, 'Liberation Mono', Menlo, monospace;
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-size: 0.9em;
        }

        pre code {
            background: none;
            padding: 0;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        th, td {
            padding: 0.75rem;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        th {
            background: var(--code-bg);
            font-weight: 600;
        }

        tr:hover {
            background: #fafbfc;
        }

        /* Lists */
        ul, ol {
            margin-bottom: 1.2rem;
            padding-left: 2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Charts */
        .chart-container {
            width: 100%;
            height: 350px;
            margin: 2rem 0;
        }

        /* Badges */
        .badge {
            display: inline-block;
            padding: 0.25rem 0.5rem;
            background: var(--accent);
            color: white;
            border-radius: 4px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-right: 0.5rem;
        }

        .badge.green {
            background: #28a745;
        }

        .badge.orange {
            background: #fd7e14;
        }

        .badge.red {
            background: #dc3545;
        }

        /* Flow diagram */
        .flow-diagram {
            background: white;
            border: 2px solid var(--border-color);
            border-radius: 8px;
            padding: 2rem;
            margin: 2rem 0;
            text-align: center;
        }

        .flow-arrow {
            font-size: 1.5rem;
            color: #666;
            margin: 0.5rem 0;
        }

        /* Responsive */
        @media (max-width: 768px) {
            h1 {
                font-size: 2rem;
            }
            body {
                padding: 2rem 1rem;
            }
        }

        /* Special sections */
        .tldr {
            background: #e8f4f8;
            border: 2px solid #0366d6;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 2rem 0;
        }

        .tldr h3 {
            margin-top: 0;
            color: #0366d6;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <h1>SimpLang: What if CUDA Wasn't Locked to NVIDIA?</h1>
    <div class="subtitle">A Transparent Compute Language for the Post-GPU Era</div>

    <div class="highlight-box">
        <strong>The Pitch:</strong> CUDA won because it gave developers control AND performance. But it locked everyone to one vendor. OpenCL tried to fix this and got neither. SimpLang gives you all three: portability, control, and performance — through inspectable MLIR compilation.
    </div>

    <div class="tldr">
        <h3>TL;DR</h3>
        <p>
            We built a compute language that's basically "CUDA for everything that has SIMD". Unlike CUDA (NVIDIA-only) or OpenCL (black box), you can see exactly how your code compiles and optimizes. On real workloads:
        </p>
        <ul>
            <li><strong>3.72x faster</strong> than GCC on byte operations</li>
            <li><strong>1.37x average speedup</strong> on array operations</li>
            <li><strong>Competitive</strong> on transformer inference</li>
        </ul>
        <p>
            The kicker? It's one person + AI building this. Not 500 NVIDIA engineers.
        </p>
    </div>

    <h2>The Portable Compute Crisis Nobody Talks About</h2>

    <p>
        Here's what happened: GPUs ate the world of compute. CUDA became the de facto standard. Everyone learned to write <code>kernel<<<blocks, threads>>>()</code> and life was good... if you could afford NVIDIA hardware.
    </p>

    <p>
        But something interesting is happening in 2025. The hardware landscape is fragmenting:
    </p>

    <ul>
        <li><strong>CPUs got serious about SIMD:</strong> AVX-512 on Intel, SVE2 on ARM, Vector extensions on RISC-V</li>
        <li><strong>NPUs are everywhere:</strong> Your phone has one. Your laptop has one. They're all different.</li>
        <li><strong>Edge killed discrete GPUs:</strong> Power budgets don't allow for 300W cards in robots/drones/IoT</li>
        <li><strong>Custom silicon is viable:</strong> Google has TPUs, Apple has Neural Engine, startups are taping out accelerators</li>
    </ul>

    <p>
        The problem? Your CUDA code doesn't run on any of this. Your OpenCL code "runs" but at 10% the performance. The industry's answer? Write different code for each target. That's insane.
    </p>

    <div class="insight-box">
        <strong>Key Insight:</strong> The right abstraction for portable compute isn't "write once, forget how it runs" (OpenCL model). It's "write once, understand how it runs anywhere" (SimpLang model).
    </div>

    <h2>How We're Different (The Technical Bet)</h2>

    <p>
        SimpLang makes a controversial technical bet: <strong>developers want to see the compilation process, not hide from it</strong>.
    </p>

    <p>
        When you write SimpLang code, it transforms through multiple intermediate representations (IRs) that you can inspect:
    </p>

    <div class="flow-diagram">
        <div><strong>Your Code</strong> (.sl file)</div>
        <div class="flow-arrow">↓</div>
        <div><strong>AST</strong> (what you wrote)</div>
        <div class="flow-arrow">↓</div>
        <div><strong>Simp Dialect</strong> (high-level ops)</div>
        <div class="flow-arrow">↓</div>
        <div><strong>Linalg + MemRef</strong> (tensor/memory ops)</div>
        <div class="flow-arrow">↓</div>
        <div><strong>SCF + Vector</strong> (loops + SIMD)</div>
        <div class="flow-arrow">↓</div>
        <div><strong>LLVM IR</strong> (almost assembly)</div>
        <div class="flow-arrow">↓</div>
        <div><strong>Native Code</strong> (x86/ARM/RISC-V)</div>
    </div>

    <p>
        At each stage, you can dump the IR, see what optimizations happened, and understand why your code is fast (or slow). This isn't a bug - it's the entire point.
    </p>

    <h3>Example: A Simple Reduction</h3>

    <pre><code>// SimpLang code
fn sum_array(var arr f64[], var n i32) -> f64 {
    var sum = 0.0;
    for (var i = 0; i < n; i = i + 1) {
        sum = sum + arr[i];
    }
    return sum;
}</code></pre>

    <p>
        With <code>--dump-mlir-passes</code>, you see it transform:
    </p>

    <pre><code>// After vectorization (simplified)
%vec_sum = vector.reduction "add" %loaded_vector
// This becomes AVX-512: vaddpd + horizontal sum</code></pre>

    <p>
        Compare this to CUDA where you write explicit parallelism, or OpenCL where optimization is a black box. SimpLang shows you the journey from algorithm to assembly.
    </p>

    <h2>Performance: Where We Win and Where We Don't (Yet)</h2>

    <p>
        Let's be honest about performance. We're not beating hand-tuned assembly. But we're doing something interesting - beating mature C++ compilers at their own game.
    </p>

    <h3>The Wins: Multi-Dimensional Arrays</h3>

    <div class="chart-container">
        <canvas id="arrayPerfChart"></canvas>
    </div>

    <p>
        On fundamental array operations, we destroy GCC and Clang:
    </p>

    <table>
        <thead>
            <tr>
                <th>Operation</th>
                <th>Data Type</th>
                <th>SimpLang</th>
                <th>GCC -O3</th>
                <th>Speedup</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Sequential Write</td>
                <td>i8 (bytes)</td>
                <td><strong>74.7 GB/s</strong></td>
                <td>20.1 GB/s</td>
                <td><span class="badge green">3.72x</span></td>
            </tr>
            <tr>
                <td>Sequential Read</td>
                <td>i8</td>
                <td><strong>42.9 GB/s</strong></td>
                <td>17.5 GB/s</td>
                <td><span class="badge green">2.45x</span></td>
            </tr>
            <tr>
                <td>2D Array Access</td>
                <td>i32</td>
                <td><strong>3.85 GB/s</strong></td>
                <td>3.42 GB/s</td>
                <td><span class="badge green">1.13x</span></td>
            </tr>
        </tbody>
    </table>

    <p>
        Why do we win? MLIR's polyhedral analysis understands array access patterns better than traditional compilers. We generate tight SIMD loops that GCC's pattern matching misses.
    </p>

    <h3>The Reality Check: Transformer Inference</h3>

    <p>
        On transformer models, we're bandwidth-limited at 7-8 GB/s (vs 80 GB/s theoretical). Why? We have a bug - we're loading/storing accumulators inside the inner loop:
    </p>

    <pre><code>// Our current (bad) codegen
for k in 0..K:
    C[i,j] = load(C[i,j])     // Unnecessary!
    C[i,j] += A[i,k] * B[k,j]
    store(C[i,j])             // Unnecessary!</code></pre>

    <p>
        Fix this one issue and we get 2.59x speedup. That's the difference between research code and production - and we know exactly where to look because our compilation is transparent.
    </p>

    <h2>The Roadmap: From CPU to Everything</h2>

    <p>
        Here's the ambitious part. We're not stopping at CPUs.
    </p>

    <div class="flow-diagram">
        <strong>2024:</strong> x86 CPU (AVX-512) <span class="badge green">✓ Done</span><br><br>
        <strong>2025:</strong> ARM CPU (Neon/SVE) <span class="badge orange">In Progress</span><br><br>
        <strong>2026:</strong> Mobile NPUs (Qualcomm/Apple) <span class="badge">Planned</span><br><br>
        <strong>2027:</strong> Custom Accelerators (RISC-V/FPGA) <span class="badge">Research</span>
    </div>

    <p>
        The beauty of MLIR? Each target is just a new dialect. We don't rewrite the compiler - we add a lowering pass. Your SimpLang code stays the same.
    </p>

    <h2>Use Cases Beyond ML (The Bigger Picture)</h2>

    <p>
        Everyone assumes we're building another ML framework. We're not. ML is one use case among many:
    </p>

    <h3>Scientific Computing</h3>
    <ul>
        <li>Molecular dynamics (particle simulations)</li>
        <li>Climate modeling (stencil computations)</li>
        <li>Computational fluid dynamics</li>
    </ul>
    <p><strong>Why SimpLang:</strong> Scientists have CPUs, not GPUs. They need portable performance.</p>

    <h3>Signal Processing</h3>
    <ul>
        <li>Software-defined radio</li>
        <li>Audio DSP (real-time filters)</li>
        <li>Radar/sonar processing</li>
    </ul>
    <p><strong>Why SimpLang:</strong> Deterministic latency matters. GPUs have scheduling jitter.</p>

    <h3>Embedded Systems</h3>
    <ul>
        <li>Robotics (sensor fusion)</li>
        <li>Automotive (ADAS)</li>
        <li>Drones (computer vision)</li>
    </ul>
    <p><strong>Why SimpLang:</strong> Power constraints eliminate GPUs. ARM CPUs with Neon are everywhere.</p>

    <h2>The Educational Angle (Teaching the Next Generation)</h2>

    <p>
        Here's something CUDA can't do: teach you how compilers actually work. SimpLang is designed to be learned, not just used.
    </p>

    <h3>A Complete Compiler Course in One Codebase</h3>

    <table>
        <thead>
            <tr>
                <th>Week</th>
                <th>Topic</th>
                <th>What Students Build</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>1-3</td>
                <td>Frontend</td>
                <td>Lexer/Parser (Flex/Bison)</td>
            </tr>
            <tr>
                <td>4-6</td>
                <td>LLVM Backend</td>
                <td>Direct AST→IR lowering</td>
            </tr>
            <tr>
                <td>7-9</td>
                <td>MLIR Backend</td>
                <td>Custom dialect + lowering</td>
            </tr>
            <tr>
                <td>10-11</td>
                <td>Optimization</td>
                <td>New pass (e.g., loop fusion)</td>
            </tr>
            <tr>
                <td>12</td>
                <td>Final Project</td>
                <td>Target new hardware</td>
            </tr>
        </tbody>
    </table>

    <p>
        Students get to see the entire stack. Not toy examples - real compilation that generates real assembly that runs real workloads. Try teaching that with CUDA's proprietary PTX.
    </p>

    <h2>Competition: Why Nobody Else Is Doing This</h2>

    <table>
        <thead>
            <tr>
                <th>Language</th>
                <th>Portability</th>
                <th>Performance</th>
                <th>Transparency</th>
                <th>Learning Curve</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>CUDA</strong></td>
                <td><span class="badge red">NVIDIA only</span></td>
                <td><span class="badge green">Excellent</span></td>
                <td><span class="badge red">Proprietary</span></td>
                <td>High</td>
            </tr>
            <tr>
                <td><strong>OpenCL</strong></td>
                <td><span class="badge green">Multi-vendor</span></td>
                <td><span class="badge orange">Variable</span></td>
                <td><span class="badge red">Black box</span></td>
                <td>Very High</td>
            </tr>
            <tr>
                <td><strong>SYCL</strong></td>
                <td><span class="badge orange">Intel focus</span></td>
                <td><span class="badge green">Good</span></td>
                <td><span class="badge orange">Partial</span></td>
                <td>High</td>
            </tr>
            <tr>
                <td><strong>Halide</strong></td>
                <td><span class="badge green">CPU/GPU</span></td>
                <td><span class="badge green">Good</span></td>
                <td><span class="badge orange">Schedule only</span></td>
                <td>Medium</td>
            </tr>
            <tr>
                <td><strong>SimpLang</strong></td>
                <td><span class="badge green">CPU + future</span></td>
                <td><span class="badge green">Competitive</span></td>
                <td><span class="badge green">Full MLIR</span></td>
                <td><strong>Low</strong></td>
            </tr>
        </tbody>
    </table>

    <p>
        We're the only ones showing you the full compilation pipeline. That's either brilliant or stupid - we'll find out.
    </p>

    <h2>Technical Deep Dive: Why We Beat GCC</h2>

    <p>
        For the compiler nerds, here's why we can beat GCC at array operations:
    </p>

    <h3>1. Polyhedral Analysis vs Pattern Matching</h3>
    <p>
        GCC uses pattern matching for vectorization. It looks for specific loop shapes and says "I know how to vectorize this."
    </p>
    <p>
        MLIR uses polyhedral analysis. It understands the mathematical structure of your loop nest and can reason about dependencies algebraically. This finds vectorization opportunities that pattern matching misses.
    </p>

    <h3>2. Type Information Preservation</h3>
    <p>
        C++ compilers lose type information after the frontend. By the time GCC is optimizing, it doesn't know if your <code>char*</code> is text or binary data.
    </p>
    <p>
        SimpLang preserves semantic types through the entire pipeline. We know <code>i8[]</code> is meant for SIMD processing and optimize accordingly.
    </p>

    <h3>3. Controlled Unrolling</h3>
    <p>
        GCC with <code>-O3 -march=native</code> goes crazy with unrolling. On AVX-512, it might unroll 16x, causing register spills and instruction cache misses.
    </p>
    <p>
        We unroll conservatively (4x) based on register pressure analysis. Less aggressive, more consistent.
    </p>

    <h2>The Accumulator Bug (A Confession)</h2>

    <p>
        Let's talk about our biggest performance bug. It's embarrassing but instructive.
    </p>

    <p>
        In our matrix multiplication, we generate this pattern:
    </p>

    <pre><code>// What we generate (bad)
for (i = 0; i < M; i++) {
    for (j = 0; j < N; j++) {
        for (k = 0; k < K; k++) {
            C[i][j] = C[i][j] + A[i][k] * B[k][j];  // Load and store every iteration!
        }
    }
}</code></pre>

    <p>
        What we should generate:
    </p>

    <pre><code>// What we should generate (good)
for (i = 0; i < M; i++) {
    for (j = 0; j < N; j++) {
        float sum = C[i][j];  // Load once
        for (k = 0; k < K; k++) {
            sum += A[i][k] * B[k][j];  // Accumulate in register
        }
        C[i][j] = sum;  // Store once
    }
}</code></pre>

    <p>
        This one fix would give us 2.59x speedup on transformers. We know exactly where the problem is because we can see the generated IR. Try debugging that in OpenCL.
    </p>

    <h2>System Details: What We're Running On</h2>

    <p>
        All benchmarks run on this beast of a machine:
    </p>

    <ul>
        <li><strong>CPU:</strong> AMD Ryzen 7 7800X3D (8 cores @ 5.05 GHz)</li>
        <li><strong>Cache:</strong> 96MB L3 (3D V-Cache!) - This is huge for bandwidth-limited workloads</li>
        <li><strong>Memory:</strong> 61 GB DDR5 (~80 GB/s theoretical)</li>
        <li><strong>SIMD:</strong> Full AVX-512 support (F, BW, DQ, CD, IFMA, VBMI)</li>
        <li><strong>OS:</strong> Pop!_OS 22.04, Linux 6.12</li>
        <li><strong>Compiler:</strong> LLVM 14, custom MLIR build</li>
    </ul>

    <p>
        That 96MB L3 cache is particularly interesting. With proper tiling, we could fit entire transformer layers in cache. We're leaving performance on the table.
    </p>

    <h2>The One-Person Angle (David vs Goliath)</h2>

    <p>
        Here's the crazy part: this is built by one person + AI assistants. Not a team at Google. Not NVIDIA's compiler division. One person.
    </p>

    <p>
        Is this sustainable? Probably not at NVIDIA's scale. But for proving the concept? For showing that portable compute with transparent compilation is possible? Absolutely.
    </p>

    <p>
        The bet is that AI coding assistants change the game. What took 100 engineers in 2010 might be doable by one person + AI in 2025. We're testing that hypothesis live.
    </p>

    <h2>Future Work: The Next 12 Months</h2>

    <h3>Q1 2025: Fix the Accumulator</h3>
    <p>One bug fix, 2.59x speedup. This is priority #1.</p>

    <h3>Q2 2025: ARM Backend</h3>
    <p>ARM Neon first (easier), then SVE2 (harder). Every phone and Mac becomes a target.</p>

    <h3>Q3 2025: Benchmarks Suite</h3>
    <p>BLAS, FFT, convolution, reduction. Show we're not just good at one thing.</p>

    <h3>Q4 2025: First NPU</h3>
    <p>Probably Qualcomm Hexagon. It's documented and has MLIR precedent.</p>

    <h2>How To Get Involved</h2>

    <p>
        SimpLang is open source. If you think portable compute matters, here's how to help:
    </p>

    <ul>
        <li><strong>Try it:</strong> Write some kernels. Break things. File bugs.</li>
        <li><strong>Benchmark it:</strong> We need more workloads. Surprise us.</li>
        <li><strong>Extend it:</strong> Pick a target (RISC-V? WebAssembly?) and add a backend.</li>
        <li><strong>Teach with it:</strong> Use it in a compilers course. Tell us what's missing.</li>
    </ul>

    <p>
        GitHub: <a href="https://github.com/simplang/simplang">github.com/simplang/simplang</a>
    </p>

    <h2>The Big Question</h2>

    <p>
        Can a transparent, portable compute language compete with CUDA's ecosystem? We don't know. But we know the current situation - vendor lock-in with CUDA, poor performance with OpenCL - isn't sustainable.
    </p>

    <p>
        As hardware fragments (CPUs, NPUs, custom silicon), we need a new abstraction. Not "write once, run anywhere" (that failed). But "write once, understand everywhere."
    </p>

    <p>
        SimpLang is our bet on that future. A future where you can see your code transform, understand the optimizations, and target any SIMD-capable processor.
    </p>

    <p>
        The alternative is writing different code for every chip. That's not a future we want to live in.
    </p>

    <div class="highlight-box" style="margin-top: 3rem;">
        <strong>The Bottom Line:</strong> We're building CUDA for everything that isn't an NVIDIA GPU. It's transparent, portable, and competitive on performance. One person + AI is building it. Join us or watch us fail spectacularly. Either way, it'll be interesting.
    </div>

    <script>
        // Array Performance Chart
        const ctx = document.getElementById('arrayPerfChart').getContext('2d');
        new Chart(ctx, {
            type: 'bar',
            data: {
                labels: ['i8 Write', 'i8 Read', 'i64 Write', 'i32 2D Array', 'f64 3D Array'],
                datasets: [{
                    label: 'SimpLang',
                    data: [74.7, 42.9, 3.33, 3.85, 3.12],
                    backgroundColor: 'rgba(111, 66, 193, 0.8)',
                    borderColor: '#6f42c1',
                    borderWidth: 2
                }, {
                    label: 'GCC -O3',
                    data: [20.1, 17.5, 3.00, 3.42, 2.98],
                    backgroundColor: 'rgba(40, 167, 69, 0.6)',
                    borderColor: '#28a745',
                    borderWidth: 2
                }, {
                    label: 'Clang -O3',
                    data: [32.4, 25.2, 2.97, 3.38, 3.01],
                    backgroundColor: 'rgba(253, 126, 20, 0.6)',
                    borderColor: '#fd7e14',
                    borderWidth: 2
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: 'Multi-dimensional Array Performance (GB/s)',
                        font: { size: 16, weight: 'bold' }
                    },
                    legend: {
                        position: 'top',
                    }
                },
                scales: {
                    y: {
                        type: 'logarithmic',
                        beginAtZero: false,
                        title: {
                            display: true,
                            text: 'Bandwidth (GB/s) - Log Scale'
                        },
                        ticks: {
                            callback: function(value) {
                                if (value === 100 || value === 50 || value === 20 || value === 10 || value === 5 || value === 2 || value === 1) {
                                    return value;
                                }
                                return null;
                            }
                        }
                    },
                    x: {
                        title: {
                            display: true,
                            text: 'Operation Type'
                        }
                    }
                }
            }
        });
    </script>
</body>
</html>